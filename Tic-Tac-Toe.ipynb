{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/alisithomas/Environments/tic-tac-toe/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Player:\n",
    "    \n",
    "    def __init__(self, name, symbol, strategy):\n",
    "        self._name = name\n",
    "        self._symbol = symbol\n",
    "        self._strategy = strategy\n",
    "    \n",
    "    def play(self, board):\n",
    "        '''make next move according to strategy'''\n",
    "        return self._strategy.next_move(board=board)\n",
    "    \n",
    "    @property\n",
    "    def symbol(self):\n",
    "        return self._symbol\n",
    "    \n",
    "    @property\n",
    "    def name(self):\n",
    "        return self._name\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        return other is not None and self._name == other._name and self._symbol == other._symbol\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash((self._name, self._symbol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Board:\n",
    "    \n",
    "    _EMPTY_SYMBOL = ' '\n",
    "    _ROWS = 3\n",
    "    _COLUMNS = _ROWS # we can't play tic tac tow on rectangular boards, can we?\n",
    "    _SIZE = _ROWS * _COLUMNS\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._board = [self._EMPTY_SYMBOL] * self._SIZE\n",
    "        self._history = []\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return ''.join([\" {} {}\".format(self._board[i], self._repr_board(i)) for i in range(self._SIZE)])\n",
    "    \n",
    "    def _repr_board(self, i):\n",
    "        max_val = self._ROWS - 1\n",
    "        return \"|\" if i % self._ROWS != max_val else \"\\n{}\\n\".format(\"-\" * (self._SIZE + max_val) if i // self._ROWS != max_val else \"\")\n",
    "        \n",
    "    def update(self, row, col, player):\n",
    "        index = row * self._ROWS + col\n",
    "        self._board[index] = player.symbol\n",
    "        self._history.append((index, player.symbol))\n",
    "    \n",
    "    def get_value_at_coordinate(self, row, col):\n",
    "        return self._board[row * self._ROWS + col]\n",
    "        \n",
    "    @property\n",
    "    def free_coordinates(self):\n",
    "        '''returns list of tuples of coordinates having empty symbols'''\n",
    "        return [(i // self._COLUMNS, i % self._COLUMNS) for i in range(self._SIZE) if self._board[i] == self._EMPTY_SYMBOL]\n",
    "\n",
    "    @property\n",
    "    def size(self):\n",
    "        return self._SIZE\n",
    "    \n",
    "    @property\n",
    "    def columns(self):\n",
    "        return self._COLUMNS\n",
    "\n",
    "    @property\n",
    "    def rows(self):\n",
    "        return self._ROWS\n",
    "\n",
    "    @property\n",
    "    def is_empty(self):\n",
    "        return len(self.free_coordinates) == self._SIZE\n",
    "    \n",
    "    @property\n",
    "    def is_full(self):\n",
    "        return len(self.free_coordinates) == 0\n",
    "    \n",
    "    @property\n",
    "    def history(self):\n",
    "        return self._history + [(-1, self._EMPTY_SYMBOL) for i in range(len(self._history), self._SIZE)]\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return self._board == other._board\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash(''.join(self._board))\n",
    "    \n",
    "    def show(self):\n",
    "        print(self.__repr__())\n",
    "        \n",
    "    def clone(self):\n",
    "        clone = Board()\n",
    "        clone._board = [self._board[i] for i in range(self._SIZE)]\n",
    "        clone._history = [(h[0], h[1]) for h in self._history]\n",
    "        return clone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResultChecker:\n",
    "    \n",
    "    def __init__(self, player_1, player_2):\n",
    "        self._player_1 = player_1\n",
    "        self._player_2 = player_2\n",
    "    \n",
    "    def check_winner(self, board):\n",
    "        rows = {i: [] for i in range(board.rows)}\n",
    "        cols = {i: [] for i in range(board.columns)}\n",
    "        diag = []\n",
    "        rdiag = []\n",
    "        \n",
    "        for i in range(board.size):\n",
    "            row = i // board.columns\n",
    "            col = i % board.columns\n",
    "            rows[row].append(board.get_value_at_coordinate(row, col))\n",
    "            cols[col].append(board.get_value_at_coordinate(row, col))\n",
    "            if row == col:\n",
    "                diag.append(board.get_value_at_coordinate(row, col))\n",
    "            if row + col == board.columns - 1:\n",
    "                rdiag.append(board.get_value_at_coordinate(row, col))\n",
    "\n",
    "        for candidate in [diag, rdiag] + [v for v in rows.values()] + [v for v in cols.values()]:\n",
    "            for player in [self._player_1, self._player_2]:\n",
    "                if candidate == [player.symbol] * board.columns:\n",
    "                    return player\n",
    "                \n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToeState:\n",
    "    \n",
    "    def __init__(self, board, reward):\n",
    "        self._board = board\n",
    "        self._reward = reward\n",
    "        self._previous = []\n",
    "        self._next = []\n",
    "    \n",
    "    @property\n",
    "    def board(self):\n",
    "        return self._board\n",
    "\n",
    "    @property\n",
    "    def reward(self):\n",
    "        return self._reward\n",
    "    \n",
    "    @property\n",
    "    def prev_states(self):\n",
    "        return self._previous\n",
    "\n",
    "    @property\n",
    "    def next_states(self):\n",
    "        return self._next\n",
    "\n",
    "    def update_reward(self, new_reward):\n",
    "        self._reward = new_reward\n",
    "    \n",
    "    def add_next_state(self, state):\n",
    "        self._next.append(state)\n",
    "    \n",
    "    def add_previous_state(self, state):\n",
    "        self._previous.append(state)\n",
    "        \n",
    "    def clone(self):\n",
    "        s = TicTacToeState(self._board.clone(), self._reward)\n",
    "        s._previous = [p for p in self._previous]\n",
    "        s._next = [n for n in self._next]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Strategy:\n",
    "    '''stragegy interface, extend and implement method next_move to have a strategy'''\n",
    "    def next_move(self, board):\n",
    "        return None\n",
    "\n",
    "class RandomStrategy(Strategy):\n",
    "\n",
    "    def next_move(self, board):\n",
    "        fc = board.free_coordinates\n",
    "        return fc[random.randint(0, len(fc) - 1)] if len(fc) > 0 else None\n",
    "\n",
    "class HumanStrategy(Strategy):\n",
    "\n",
    "    def next_move(self, board):\n",
    "        choice_str = raw_input(\"Your turn (ex: 1,2)>\")\n",
    "        coordinates_str = choice_str.split(\",\")\n",
    "        return tuple(map(int, coordinates_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReinforcementLearningStrategy(Strategy):\n",
    "    \n",
    "    def __init__(self, explore=0.15, alpha=0.06):\n",
    "        self._explore = explore\n",
    "        self._alpha = alpha\n",
    "        self._state_map = dict()\n",
    "        self._choices = []\n",
    "        self._target_player = None\n",
    "        \n",
    "    def new_match(self):\n",
    "        self._choices = []\n",
    "\n",
    "    def generate_all_states(self, player_1, player_2, target_player):\n",
    "        self._target_player = target_player\n",
    "        result_checker = ResultChecker(player_1, player_2)\n",
    "        \n",
    "        initial_state = TicTacToeState(board=Board(), reward=0)\n",
    "        self._state_map[initial_state.board] = initial_state\n",
    "        stack = [(initial_state, player_1), (initial_state, player_2)]\n",
    "        \n",
    "        while len(stack) > 0:            \n",
    "            cur_state, cur_player = stack.pop()\n",
    "            cur_board = cur_state.board\n",
    "            \n",
    "            if result_checker.check_winner(cur_board):\n",
    "                continue\n",
    "            \n",
    "            for row, col in cur_board.free_coordinates:\n",
    "                new_board = cur_board.clone()\n",
    "                new_board.update(row, col, cur_player)\n",
    "                \n",
    "                new_state = None\n",
    "                \n",
    "                if new_board in self._state_map:\n",
    "                    new_state = self._state_map[new_board]\n",
    "                else:\n",
    "                    winner = result_checker.check_winner(new_board)\n",
    "                    \n",
    "                    if winner == target_player:\n",
    "                        reward = 1.0\n",
    "                    elif winner is not None: # Player 2 won\n",
    "                        reward = 0.0\n",
    "                    elif len(new_board.free_coordinates) == 0: # tie\n",
    "                        reward = 0.0\n",
    "                    else:\n",
    "                        reward = 0.1\n",
    "                        \n",
    "                    new_state = TicTacToeState(board=new_board, reward=reward)\n",
    "                    self._state_map[new_state.board] = new_state\n",
    "                    \n",
    "                new_state.add_previous_state(cur_state)\n",
    "                cur_state.add_next_state(new_state)                \n",
    "                stack.append((new_state, player_1 if cur_player != player_1 else player_2))\n",
    "                \n",
    "    def next_move(self, board):\n",
    "        next_state, to_update = self._choose_next_state(board)\n",
    "        \n",
    "        if len(self._choices) > 0 and to_update:\n",
    "            self._update_reward(prev_state=self._choices[-1], next_state=next_state)\n",
    "            \n",
    "        self._choices.append(next_state)\n",
    "        chosen_move = set(board.free_coordinates) - set(next_state.board.free_coordinates)\n",
    "        return chosen_move.pop() if len(chosen_move) > 0 else None\n",
    "        \n",
    "    \n",
    "    def _choose_next_state(self, board):\n",
    "        fc = board.free_coordinates\n",
    "        cur_state = self._state_map[board]\n",
    "        \n",
    "        if len(fc) == 0 or len(cur_state.next_states) == 0:\n",
    "            return cur_state, True\n",
    "        elif random.random() < self._explore: # Exploring\n",
    "            choice = fc[random.randint(0, len(fc) - 1)]\n",
    "            new_board = board.clone()\n",
    "            new_board.update(choice[0], choice[1], self._target_player)\n",
    "            return self._state_map[new_board], False\n",
    "        else: # Exploiting\n",
    "            state_with_max_reward = max(cur_state.next_states, key=lambda s: s.reward)\n",
    "            return state_with_max_reward, True\n",
    "    \n",
    "    def _update_reward(self, prev_state, next_state):\n",
    "        new_reward = prev_state.reward + self._alpha * (next_state.reward - prev_state.reward)\n",
    "        # print(\"Updating reward from {} to: {}\".format(prev_state.reward(), new_reward))\n",
    "        prev_state.update_reward(new_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepLearningStrategy(Strategy):\n",
    "    def __init__(self):\n",
    "        self._model = self.init_nn()\n",
    "        self._current_player = None\n",
    "\n",
    "    @staticmethod\n",
    "    def init_nn():\n",
    "        board = Board()\n",
    "        PLAYER_DIM = 2 # current player + target player\n",
    "        BOARD_DIM = board.size # i.e. 9\n",
    "        POSS_MOVE_DIM = BOARD_DIM # i.e. the next board, so same as board size\n",
    "        INPUT_DIM = PLAYER_DIM + BOARD_DIM + POSS_MOVE_DIM # turn, board\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Dense(2 * INPUT_DIM, input_dim=INPUT_DIM))\n",
    "        model.add(Dense(INPUT_DIM))\n",
    "        model.add(Dense(BOARD_DIM))\n",
    "        model.add(Dense(1)) # predicting if the move was a 'winning' move or not\n",
    "        model.add(Activation('softmax'))\n",
    "        model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "    def generate_all_states(self, player_1, player_2, target_player):\n",
    "        # storing current player, useful?\n",
    "        self._current_player = target_player\n",
    "        result_checker = ResultChecker(player_1, player_2)\n",
    "        initial_state = TicTacToeState(board=Board(), reward=0)\n",
    "        # used to check if a state has been evaluated (maybe?)\n",
    "        state_map = {\n",
    "            initial_state.board: initial_state\n",
    "        }\n",
    "        stack = [(initial_state, player_1), (initial_state, player_2)]\n",
    "        \n",
    "        data = []\n",
    "        labels = []\n",
    "        while len(stack) > 0:            \n",
    "            cur_state, cur_player = stack.pop()\n",
    "            cur_board = cur_state.board\n",
    "            \n",
    "            if result_checker.check_winner(cur_board):\n",
    "                continue\n",
    "            \n",
    "            for row, col in cur_board.free_coordinates:\n",
    "                new_board = cur_board.clone()\n",
    "                new_board.update(row, col, cur_player)\n",
    "                \n",
    "                new_state = None\n",
    "                \n",
    "                if new_board in state_map:\n",
    "                    new_state = state_map[new_board]\n",
    "                else:\n",
    "                    winner = result_checker.check_winner(new_board)\n",
    "                    \n",
    "                    if winner == target_player:\n",
    "                        # we won, yay!\n",
    "                        reward = 1.0\n",
    "                    elif winner is not None: \n",
    "                        # they won, booh!\n",
    "                        reward = 0.0\n",
    "                    elif len(new_board.free_coordinates) == 0: \n",
    "                        # it's a tie\n",
    "                        reward = 0.0\n",
    "                    else:\n",
    "                        # still playing\n",
    "                        reward = 0.1\n",
    "                    \n",
    "                    new_state = TicTacToeState(board=new_board, reward=reward)\n",
    "                    \n",
    "                stack.append((new_state, player_1 if cur_player != player_1 else player_2))\n",
    "                data.append(np.asarray(\n",
    "                    [hash(cur_player), hash(target_player)] + \n",
    "                    [hash(i) for i in cur_board.history] + \n",
    "                    [hash(i) for i in new_board.history]))\n",
    "                labels.append(np.asarray([reward]))\n",
    "                state_map[new_state.board] = new_state\n",
    "\n",
    "        self._model.fit(data, labels, nb_epoch=3, validation_split=0.05)\n",
    "\n",
    "    def next_move(self, board):\n",
    "        for row, col in board.free_coordinates:\n",
    "            pass\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Match:\n",
    "    \n",
    "    def __init__(self, board, player_1, player_2):\n",
    "        self._board = board\n",
    "        self._player_1 = player_1\n",
    "        self._player_2 = player_2\n",
    "        self._result_checker = ResultChecker(player_1, player_2)\n",
    "    \n",
    "    def play(self, show=True):\n",
    "        \n",
    "        if not self._board.is_empty:\n",
    "            raise Exception(\"Board is not empty!\\n{}\".format(self._board))\n",
    "        \n",
    "        cur_player = self._select_first_player(show)\n",
    "        while not self._board.is_full:\n",
    "            choice = cur_player.play(board=board)\n",
    "            if show:\n",
    "                print(\"Player {} ({}) has chosen {}\".format(cur_player.name, cur_player.symbol, choice))\n",
    "            self._board.update(choice[0], choice[1], cur_player)\n",
    "            if show:\n",
    "                self._board.show()\n",
    "            winner = self._result_checker.check_winner(self._board)\n",
    "            if winner:\n",
    "                player_1.play(board=board)\n",
    "                player_2.play(board=board)\n",
    "                break\n",
    "                \n",
    "            cur_player = self._player_1 if cur_player == self._player_2 else self._player_2\n",
    "        \n",
    "        winner = self._result_checker.check_winner(self._board)\n",
    "        if winner:\n",
    "            if show:\n",
    "                print(\"Player {} ({}) won the game! :) \".format(winner.name, winner.symbol))\n",
    "        else:\n",
    "            player_1.play(board=board)\n",
    "            player_2.play(board=board)\n",
    "            if show:\n",
    "                print(\"It's a tie :D\")\n",
    "        return winner\n",
    "    \n",
    "    def _select_first_player(self, show):\n",
    "        if show:\n",
    "            print(\"Flipping a coin to decide which player will start the match...\")\n",
    "        first_player = self._player_1 if random.randint(0, 1) == 0 else self._player_2\n",
    "        if show:\n",
    "            print(\"{} will start the game\".format(first_player.name))\n",
    "        return first_player\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "rls_1 = ReinforcementLearningStrategy()\n",
    "rls_2 = ReinforcementLearningStrategy()\n",
    "dls_1 = DeepLearningStrategy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 1 array(s), but instead got the following list of 1099890 arrays: [array([ 7993904492217295360,  7993904492217295360, -6635925921834205456,\n       -6635925921834205456, -6635925921834205456, -6635925921834205456,\n       -6635925921834205456, -6635925921834205456, -6...",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-119-c5acd732fd82>\u001b[0m in \u001b[0;36mgenerate_all_states\u001b[0;34m(self, player_1, player_2, target_player)\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0mstate_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboard\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnext_move\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboard\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Environments/tic-tac-toe/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    958\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 960\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m~/Environments/tic-tac-toe/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1579\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1580\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1581\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1582\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1583\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Environments/tic-tac-toe/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size)\u001b[0m\n\u001b[1;32m   1412\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1413\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1414\u001b[0;31m                                     exception_prefix='input')\n\u001b[0m\u001b[1;32m   1415\u001b[0m         y = _standardize_input_data(y, self._feed_output_names,\n\u001b[1;32m   1416\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Environments/tic-tac-toe/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m     86\u001b[0m                                  \u001b[0;34m'the following list of '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                                  \u001b[0;34m' arrays: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                                  '...')\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 1 array(s), but instead got the following list of 1099890 arrays: [array([ 7993904492217295360,  7993904492217295360, -6635925921834205456,\n       -6635925921834205456, -6635925921834205456, -6635925921834205456,\n       -6635925921834205456, -6635925921834205456, -6..."
     ]
    }
   ],
   "source": [
    "time dls_1.generate_all_states(player_1=player_1, player_2=player_2, target_player=player_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_1 = Player(name='first', symbol='X', strategy=rls_1)\n",
    "player_2 = Player(name='second', symbol='O', strategy=rls_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 39.4 s, sys: 106 ms, total: 39.5 s\n",
      "Wall time: 39.8 s\n"
     ]
    }
   ],
   "source": [
    "time rls_1.generate_all_states(player_1=player_1, player_2=player_2, target_player=player_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 39 s, sys: 81.6 ms, total: 39.1 s\n",
      "Wall time: 39.3 s\n"
     ]
    }
   ],
   "source": [
    "time rls_2.generate_all_states(player_1=player_1, player_2=player_2, target_player=player_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MATCH: 0\n",
      "MATCH: 1000\n",
      "MATCH: 2000\n",
      "MATCH: 3000\n",
      "MATCH: 4000\n",
      "MATCH: 5000\n",
      "MATCH: 6000\n",
      "MATCH: 7000\n",
      "MATCH: 8000\n",
      "MATCH: 9000\n"
     ]
    }
   ],
   "source": [
    "matches = 10000\n",
    "for i in range(matches):\n",
    "    if i % 1000 == 0:\n",
    "        print(\"MATCH: {}\".format(i))\n",
    "    board = Board()\n",
    "    rls_1.new_match()\n",
    "    rls_2.new_match()\n",
    "    match = Match(board=board, player_1=player_1, player_2=player_2)\n",
    "    match.play(show=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "human_player = Player(name='Human', symbol='O', strategy=HumanStrategy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flipping a coin to decide which player will start the match...\n",
      "first will start the game\n",
      "Player first (X) has chosen (1, 1)\n",
      "   |   |   \n",
      "-----------\n",
      "   | X |   \n",
      "-----------\n",
      "   |   |   \n",
      "\n",
      "\n",
      "Your turn (ex: 1,2)>2,1\n",
      "Player Human (O) has chosen (2, 1)\n",
      "   |   |   \n",
      "-----------\n",
      "   | X |   \n",
      "-----------\n",
      "   | O |   \n",
      "\n",
      "\n",
      "Player first (X) has chosen (0, 0)\n",
      " X |   |   \n",
      "-----------\n",
      "   | X |   \n",
      "-----------\n",
      "   | O |   \n",
      "\n",
      "\n",
      "Your turn (ex: 1,2)>2,2\n",
      "Player Human (O) has chosen (2, 2)\n",
      " X |   |   \n",
      "-----------\n",
      "   | X |   \n",
      "-----------\n",
      "   | O | O \n",
      "\n",
      "\n",
      "Player first (X) has chosen (2, 0)\n",
      " X |   |   \n",
      "-----------\n",
      "   | X |   \n",
      "-----------\n",
      " X | O | O \n",
      "\n",
      "\n",
      "Your turn (ex: 1,2)>0, 2\n",
      "Player Human (O) has chosen (0, 2)\n",
      " X |   | O \n",
      "-----------\n",
      "   | X |   \n",
      "-----------\n",
      " X | O | O \n",
      "\n",
      "\n",
      "Player first (X) has chosen (1, 0)\n",
      " X |   | O \n",
      "-----------\n",
      " X | X |   \n",
      "-----------\n",
      " X | O | O \n",
      "\n",
      "\n",
      "Player first (X) won the game! :) \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.Player instance at 0x1096ab320>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "board = Board()\n",
    "match = Match(board=board, player_1=player_1, player_2=human_player)\n",
    "match.play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
