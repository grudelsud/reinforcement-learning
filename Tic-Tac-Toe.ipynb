{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/alisithomas/Environments/tic-tac-toe/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Player:\n",
    "    \n",
    "    def __init__(self, name, symbol, strategy):\n",
    "        self._name = name\n",
    "        self._symbol = symbol\n",
    "        self._strategy = strategy\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return '{} [{}]'.format(self._name, self._symbol)\n",
    "\n",
    "    def play(self, board):\n",
    "        '''make next move according to strategy'''\n",
    "        return self._strategy.next_move(board=board)\n",
    "    \n",
    "    @property\n",
    "    def symbol(self):\n",
    "        return self._symbol\n",
    "    \n",
    "    @property\n",
    "    def name(self):\n",
    "        return self._name\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        return other is not None and self._name == other._name and self._symbol == other._symbol\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash((self._name, self._symbol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Board:\n",
    "    \n",
    "    _EMPTY_SYMBOL = ' '\n",
    "    _ROWS = 3\n",
    "    _COLUMNS = _ROWS # we can't play tic tac tow on rectangular boards, can we?\n",
    "    _SIZE = _ROWS * _COLUMNS\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._board = [self._EMPTY_SYMBOL] * self._SIZE\n",
    "        self._history = []\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return ''.join([\" {} {}\".format(self._board[i], self._repr_board(i)) for i in range(self._SIZE)])\n",
    "    \n",
    "    def _repr_board(self, i):\n",
    "        max_val = self._ROWS - 1\n",
    "        return \"|\" if i % self._ROWS != max_val else \"\\n{}\\n\".format(\"-\" * (self._SIZE + max_val) if i // self._ROWS != max_val else \"\")\n",
    "        \n",
    "    def update(self, row, col, player):\n",
    "        index = row * self._ROWS + col\n",
    "        self._board[index] = player.symbol\n",
    "        self._history.append((index, player.symbol))\n",
    "    \n",
    "    def get_value_at_coordinate(self, row, col):\n",
    "        return self._board[row * self._ROWS + col]\n",
    "        \n",
    "    @property\n",
    "    def free_coordinates(self):\n",
    "        '''returns list of tuples of coordinates having empty symbols'''\n",
    "        return [\n",
    "            (i // self._COLUMNS, i % self._COLUMNS) \n",
    "            for i in range(self._SIZE) if self._board[i] == self._EMPTY_SYMBOL\n",
    "        ]\n",
    "\n",
    "    @property\n",
    "    def size(self):\n",
    "        return self._SIZE\n",
    "    \n",
    "    @property\n",
    "    def columns(self):\n",
    "        return self._COLUMNS\n",
    "\n",
    "    @property\n",
    "    def rows(self):\n",
    "        return self._ROWS\n",
    "\n",
    "    @property\n",
    "    def is_empty(self):\n",
    "        return len(self.free_coordinates) == self._SIZE\n",
    "    \n",
    "    @property\n",
    "    def is_full(self):\n",
    "        return len(self.free_coordinates) == 0\n",
    "    \n",
    "    @property\n",
    "    def history(self):\n",
    "        return self._history + [(-1, self._EMPTY_SYMBOL) for i in range(len(self._history), self._SIZE)]\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return self._board == other._board\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash(''.join(self._board))\n",
    "    \n",
    "    def show(self):\n",
    "        print(self.__repr__())\n",
    "        \n",
    "    def clone(self):\n",
    "        clone = Board()\n",
    "        clone._board = [self._board[i] for i in range(self._SIZE)]\n",
    "        clone._history = [(h[0], h[1]) for h in self._history]\n",
    "        return clone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResultChecker:\n",
    "    \n",
    "    def __init__(self, player_1, player_2):\n",
    "        self._player_1 = player_1\n",
    "        self._player_2 = player_2\n",
    "    \n",
    "    def check_winner(self, board):\n",
    "        rows = {i: [] for i in range(board.rows)}\n",
    "        cols = {i: [] for i in range(board.columns)}\n",
    "        diag = []\n",
    "        rdiag = []\n",
    "        \n",
    "        for i in range(board.size):\n",
    "            row = i // board.columns\n",
    "            col = i % board.columns\n",
    "            rows[row].append(board.get_value_at_coordinate(row, col))\n",
    "            cols[col].append(board.get_value_at_coordinate(row, col))\n",
    "            if row == col:\n",
    "                diag.append(board.get_value_at_coordinate(row, col))\n",
    "            if row + col == board.columns - 1:\n",
    "                rdiag.append(board.get_value_at_coordinate(row, col))\n",
    "\n",
    "        for candidate in [diag, rdiag] + [v for v in rows.values()] + [v for v in cols.values()]:\n",
    "            for player in [self._player_1, self._player_2]:\n",
    "                if candidate == [player.symbol] * board.columns:\n",
    "                    return player\n",
    "                \n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToeState:\n",
    "    \n",
    "    def __init__(self, board, reward):\n",
    "        self._board = board\n",
    "        self._reward = reward\n",
    "        self._previous = []\n",
    "        self._next = []\n",
    "    \n",
    "    @property\n",
    "    def board(self):\n",
    "        return self._board\n",
    "\n",
    "    @property\n",
    "    def reward(self):\n",
    "        return self._reward\n",
    "    \n",
    "    @property\n",
    "    def prev_states(self):\n",
    "        return self._previous\n",
    "\n",
    "    @property\n",
    "    def next_states(self):\n",
    "        return self._next\n",
    "\n",
    "    def update_reward(self, new_reward):\n",
    "        self._reward = new_reward\n",
    "    \n",
    "    def add_next_state(self, state):\n",
    "        self._next.append(state)\n",
    "    \n",
    "    def add_previous_state(self, state):\n",
    "        self._previous.append(state)\n",
    "        \n",
    "    def clone(self):\n",
    "        s = TicTacToeState(self._board.clone(), self._reward)\n",
    "        s._previous = [p for p in self._previous]\n",
    "        s._next = [n for n in self._next]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Strategy:\n",
    "    '''stragegy interface, extend and implement method next_move to have a strategy'''\n",
    "    def next_move(self, board):\n",
    "        return None\n",
    "\n",
    "class RandomStrategy(Strategy):\n",
    "\n",
    "    def next_move(self, board):\n",
    "        fc = board.free_coordinates\n",
    "        return fc[random.randint(0, len(fc) - 1)] if len(fc) > 0 else None\n",
    "\n",
    "class HumanStrategy(Strategy):\n",
    "\n",
    "    def next_move(self, board):\n",
    "        choice_str = input(\"Your turn (ex: 1,2)>\")\n",
    "        coordinates_str = choice_str.split(\",\")\n",
    "        return tuple(map(int, coordinates_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReinforcementLearningStrategy(Strategy):\n",
    "    \n",
    "    def __init__(self, explore=0.15, alpha=0.06):\n",
    "        self._explore = explore\n",
    "        self._alpha = alpha\n",
    "        self._state_map = dict()\n",
    "        self._choices = []\n",
    "        self._target_player = None\n",
    "        \n",
    "    def new_match(self):\n",
    "        self._choices = []\n",
    "\n",
    "    def generate_all_states(self, player_1, player_2, target_player):\n",
    "        self._target_player = target_player\n",
    "        result_checker = ResultChecker(player_1, player_2)\n",
    "        \n",
    "        initial_state = TicTacToeState(board=Board(), reward=0)\n",
    "        self._state_map[initial_state.board] = initial_state\n",
    "        stack = [(initial_state, player_1), (initial_state, player_2)]\n",
    "        \n",
    "        while len(stack) > 0:            \n",
    "            cur_state, cur_player = stack.pop()\n",
    "            cur_board = cur_state.board\n",
    "            \n",
    "            if result_checker.check_winner(cur_board):\n",
    "                continue\n",
    "            \n",
    "            for row, col in cur_board.free_coordinates:\n",
    "                new_board = cur_board.clone()\n",
    "                new_board.update(row, col, cur_player)\n",
    "                \n",
    "                new_state = None\n",
    "                \n",
    "                if new_board in self._state_map:\n",
    "                    new_state = self._state_map[new_board]\n",
    "                else:\n",
    "                    winner = result_checker.check_winner(new_board)\n",
    "                    \n",
    "                    if winner == target_player:\n",
    "                        reward = 1.0\n",
    "                    elif winner is not None: # Player 2 won\n",
    "                        reward = 0.0\n",
    "                    elif len(new_board.free_coordinates) == 0: # tie\n",
    "                        reward = 0.0\n",
    "                    else:\n",
    "                        reward = 0.1\n",
    "                        \n",
    "                    new_state = TicTacToeState(board=new_board, reward=reward)\n",
    "                    self._state_map[new_state.board] = new_state\n",
    "                    \n",
    "                new_state.add_previous_state(cur_state)\n",
    "                cur_state.add_next_state(new_state)                \n",
    "                stack.append((new_state, player_1 if cur_player != player_1 else player_2))\n",
    "                \n",
    "    def next_move(self, board):\n",
    "        next_state, to_update = self._choose_next_state(board)\n",
    "        \n",
    "        if len(self._choices) > 0 and to_update:\n",
    "            self._update_reward(prev_state=self._choices[-1], next_state=next_state)\n",
    "            \n",
    "        self._choices.append(next_state)\n",
    "        chosen_move = set(board.free_coordinates) - set(next_state.board.free_coordinates)\n",
    "        return chosen_move.pop() if len(chosen_move) > 0 else None\n",
    "        \n",
    "    \n",
    "    def _choose_next_state(self, board):\n",
    "        fc = board.free_coordinates\n",
    "        cur_state = self._state_map[board]\n",
    "        \n",
    "        if len(fc) == 0 or len(cur_state.next_states) == 0:\n",
    "            return cur_state, True\n",
    "        elif random.random() < self._explore: # Exploring\n",
    "            choice = fc[random.randint(0, len(fc) - 1)]\n",
    "            new_board = board.clone()\n",
    "            new_board.update(choice[0], choice[1], self._target_player)\n",
    "            return self._state_map[new_board], False\n",
    "        else: # Exploiting\n",
    "            state_with_max_reward = max(cur_state.next_states, key=lambda s: s.reward)\n",
    "            return state_with_max_reward, True\n",
    "    \n",
    "    def _update_reward(self, prev_state, next_state):\n",
    "        new_reward = prev_state.reward + self._alpha * (next_state.reward - prev_state.reward)\n",
    "        # print(\"Updating reward from {} to: {}\".format(prev_state.reward(), new_reward))\n",
    "        prev_state.update_reward(new_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepLearningStrategy(Strategy):\n",
    "    def __init__(self):\n",
    "        self._model = self.init_nn()\n",
    "        self._current_player = None\n",
    "\n",
    "    @staticmethod\n",
    "    def init_nn():\n",
    "        board = Board()\n",
    "        INPUT_DIM = 2 + board.size + board.size # player (current + target) + current board + next move\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Dense(2 * INPUT_DIM, input_dim=INPUT_DIM))\n",
    "        model.add(Dense(INPUT_DIM))\n",
    "        model.add(Dense(board.size))\n",
    "        model.add(Dense(1)) # predicting if the move was a 'winning' move or not\n",
    "        model.add(Activation('softmax'))\n",
    "        model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "    def generate_all_states(self, player_1, player_2, target_player):\n",
    "        # storing current player, useful?\n",
    "        self._current_player = target_player\n",
    "        result_checker = ResultChecker(player_1, player_2)\n",
    "        initial_state = TicTacToeState(board=Board(), reward=0)\n",
    "        # used to check if a state has been evaluated (maybe?)\n",
    "        state_map = {\n",
    "            initial_state.board: initial_state\n",
    "        }\n",
    "        stack = [(initial_state, player_1), (initial_state, player_2)]\n",
    "        \n",
    "        data = []\n",
    "        labels = []\n",
    "        while len(stack) > 0:            \n",
    "            cur_state, cur_player = stack.pop()\n",
    "            cur_board = cur_state.board\n",
    "            \n",
    "            if result_checker.check_winner(cur_board):\n",
    "                continue\n",
    "            \n",
    "            for row, col in cur_board.free_coordinates:\n",
    "                new_board = cur_board.clone()\n",
    "                new_board.update(row, col, cur_player)\n",
    "                \n",
    "                new_state = None\n",
    "                \n",
    "                if new_board in state_map:\n",
    "                    new_state = state_map[new_board]\n",
    "                else:\n",
    "                    winner = result_checker.check_winner(new_board)\n",
    "                    \n",
    "                    if winner == target_player:\n",
    "                        # we won, yay!\n",
    "                        reward = 1.0\n",
    "                    elif winner is not None: \n",
    "                        # they won, booh!\n",
    "                        reward = 0.0\n",
    "                    elif len(new_board.free_coordinates) == 0: \n",
    "                        # it's a tie\n",
    "                        reward = 0.0\n",
    "                    else:\n",
    "                        # still playing\n",
    "                        reward = 0.1\n",
    "                    \n",
    "                    new_state = TicTacToeState(board=new_board, reward=reward)\n",
    "                    \n",
    "                stack.append((new_state, player_1 if cur_player != player_1 else player_2))\n",
    "                data.append([hash(cur_player), hash(target_player)] + \n",
    "                        [hash(i) for i in cur_board.history] +\n",
    "                        [hash(i) for i in new_board.history])\n",
    "                labels.append([reward])\n",
    "                state_map[new_state.board] = new_state\n",
    "\n",
    "        self._model.fit(\n",
    "            x=np.asarray(data),\n",
    "            y=np.asarray(labels),\n",
    "            epochs=1,\n",
    "            validation_split=0.33)\n",
    "\n",
    "    def next_move(self, board):\n",
    "        results = {}\n",
    "        for row, col in board.free_coordinates:\n",
    "            new_board = board.clone()\n",
    "            new_board.update(row, col, self._current_player)\n",
    "            x = np.asarray([\n",
    "                [hash(self._current_player), hash(self._current_player)] + \n",
    "                [hash(i) for i in board.history] + \n",
    "                [hash(i) for i in new_board.history]\n",
    "            ])\n",
    "            results[(row, col)] = self._model.predict(x)[0][0]\n",
    "        out = (None, 0)\n",
    "        for pos, val in results.items():\n",
    "            if val > out[1]:\n",
    "                out = ((row, col), val)\n",
    "        return out[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Match:\n",
    "    \n",
    "    def __init__(self, board, player_1, player_2):\n",
    "        self._board = board\n",
    "        self._player_1 = player_1\n",
    "        self._player_2 = player_2\n",
    "        self._result_checker = ResultChecker(player_1, player_2)\n",
    "    \n",
    "    def play(self, show=True):\n",
    "        \n",
    "        if not self._board.is_empty:\n",
    "            raise Exception(\"Board is not empty!\\n{}\".format(self._board))\n",
    "        \n",
    "        cur_player = self._select_first_player(show)\n",
    "        while not self._board.is_full:\n",
    "            choice = cur_player.play(board=self._board)\n",
    "            if show:\n",
    "                print(\"Player {} ({}) has chosen {}\".format(cur_player.name, cur_player.symbol, choice))\n",
    "            self._board.update(choice[0], choice[1], cur_player)\n",
    "            if show:\n",
    "                self._board.show()\n",
    "            winner = self._result_checker.check_winner(self._board)\n",
    "            if winner:\n",
    "                # not sure why this, william?\n",
    "                player_1.play(board=self._board)\n",
    "                player_2.play(board=self._board)\n",
    "                break\n",
    "                \n",
    "            cur_player = self._player_1 if cur_player == self._player_2 else self._player_2\n",
    "        \n",
    "        winner = self._result_checker.check_winner(self._board)\n",
    "        if winner:\n",
    "            if show:\n",
    "                print(\"Player {} ({}) won the game! :) \".format(winner.name, winner.symbol))\n",
    "        else:\n",
    "            player_1.play(board=self._board)\n",
    "            player_2.play(board=self._board)\n",
    "            if show:\n",
    "                print(\"It's a tie :D\")\n",
    "\n",
    "        return winner\n",
    "    \n",
    "    def _select_first_player(self, show):\n",
    "        if show:\n",
    "            print(\"Flipping a coin to decide which player will start the match...\")\n",
    "        first_player = self._player_1 if random.randint(0, 1) == 0 else self._player_2\n",
    "        if show:\n",
    "            print(\"{} will start the game\".format(first_player.name))\n",
    "        return first_player\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "rls_1 = ReinforcementLearningStrategy()\n",
    "rls_2 = ReinforcementLearningStrategy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls_1 = DeepLearningStrategy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_1 = Player(name='RL 1', symbol='X', strategy=rls_1)\n",
    "player_2 = Player(name='RL 2', symbol='O', strategy=rls_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_3 = Player(name='DL 1', symbol='O', strategy=dls_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 736926 samples, validate on 362964 samples\n",
      "Epoch 1/1\n",
      "736926/736926 [==============================] - 58s 78us/step - loss: 14.1262 - acc: 0.0175 - val_loss: 14.3658 - val_acc: 0.0025\n",
      "CPU times: user 1min 56s, sys: 10.1 s, total: 2min 6s\n",
      "Wall time: 1min 49s\n"
     ]
    }
   ],
   "source": [
    "time dls_1.generate_all_states(player_1=player_1, player_2=player_3, target_player=player_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 56 s, sys: 392 ms, total: 56.4 s\n",
      "Wall time: 1min 1s\n"
     ]
    }
   ],
   "source": [
    "time rls_1.generate_all_states(player_1=player_1, player_2=player_2, target_player=player_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 39.5 s, sys: 161 ms, total: 39.6 s\n",
      "Wall time: 40 s\n"
     ]
    }
   ],
   "source": [
    "time rls_2.generate_all_states(player_1=player_1, player_2=player_2, target_player=player_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MATCH: 0\n",
      "{'count': 1000, 'boards': {1: {'players': {'player_1': RL 1 [X], 'player_2': RL 2 [O]}, 'wins': {RL 1 [X]: 316, RL 2 [O]: 598}}, 2: {'players': {'player_1': RL 1 [X], 'player_2': DL 1 [O]}, 'wins': {RL 1 [X]: 893, DL 1 [O]: 105}}}}\n"
     ]
    }
   ],
   "source": [
    "matches = {\n",
    "    'count': 1000, \n",
    "    'boards': {\n",
    "        1: {'players': {'player_1': player_1, 'player_2': player_2}, 'wins': {player_1: 0, player_2: 0}},\n",
    "        2: {'players': {'player_1': player_1, 'player_2': player_3}, 'wins': {player_1: 0, player_3: 0}},\n",
    "    }\n",
    "}\n",
    "for i in range(matches['count']):\n",
    "    if i % 1000 == 0:\n",
    "        print(\"MATCH: {}\".format(i))\n",
    "    board1 = Board()\n",
    "    board2 = Board()\n",
    "    rls_1.new_match()\n",
    "    rls_2.new_match()\n",
    "    # reinf vs reinf\n",
    "    match = Match(board=board1, **matches['boards'][1]['players'])\n",
    "    winner = match.play(show=False)\n",
    "    if winner:\n",
    "        matches['boards'][1]['wins'][winner] += 1\n",
    "    # reinf vs deep\n",
    "    match = Match(board=board2, **matches['boards'][2]['players'])\n",
    "    winner = match.play(show=False)\n",
    "    if winner:\n",
    "        matches['boards'][2]['wins'][winner] += 1\n",
    "\n",
    "print(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flipping a coin to decide which player will start the match...\n",
      "Human will start the game\n",
      "Your turn (ex: 1,2)>1,1\n",
      "Player Human (X) has chosen (1, 1)\n",
      "   |   |   \n",
      "-----------\n",
      "   | X |   \n",
      "-----------\n",
      "   |   |   \n",
      "\n",
      "\n",
      "Player DL 1 (O) has chosen (2, 2)\n",
      "   |   |   \n",
      "-----------\n",
      "   | X |   \n",
      "-----------\n",
      "   |   | O \n",
      "\n",
      "\n",
      "Your turn (ex: 1,2)>0,2\n",
      "Player Human (X) has chosen (0, 2)\n",
      "   |   | X \n",
      "-----------\n",
      "   | X |   \n",
      "-----------\n",
      "   |   | O \n",
      "\n",
      "\n",
      "Player DL 1 (O) has chosen (2, 1)\n",
      "   |   | X \n",
      "-----------\n",
      "   | X |   \n",
      "-----------\n",
      "   | O | O \n",
      "\n",
      "\n",
      "Your turn (ex: 1,2)>2,0\n",
      "Player Human (X) has chosen (2, 0)\n",
      "   |   | X \n",
      "-----------\n",
      "   | X |   \n",
      "-----------\n",
      " X | O | O \n",
      "\n",
      "\n",
      "Player Human (X) won the game! :) \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Human [X]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "human_player = Player(name='Human', symbol='X', strategy=HumanStrategy())\n",
    "board = Board()\n",
    "match = Match(board=board, player_1=player_3, player_2=human_player)\n",
    "match.play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
